---
---


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{restuccia2022toward,
  abbr={Journal},
  abstract={Wireless systems such as the Internet of Things (IoT) are changing the way we interact with the cyber and the physical world. As IoT systems become more and more pervasive, it is imperative to design wireless protocols that can effectively and efficiently support IoT devices and operations. On the other hand, today's IoT wireless systems are based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. In this paper, we introduce the new notion of a deep learning-based polymorphic IoT receiver, able to reconfigure its waveform demodulation strategy itself in real time, based on the inferred waveform parameters. Our key innovation is the introduction of a novel embedded deep learning architecture that enables the solution of waveform inference problems, which is then integrated into a generalized hardware/software architecture with radio components and signal processing. Our polymorphic wireless receiver is prototyped on a custom-made software-defined radio platform. We show through extensive over-the-air experiments that the system achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible.},


  title={Toward Polymorphic Internet of Things Receivers Through Real-Time Waveform-Level Deep Learning},
  author={Restuccia, Francesco and Melodia, Tommaso},
  journal={GetMobile: Mobile Computing and Communications},
  volume={25},
  html={https://dl.acm.org/doi/abs/10.1145/3511285.3511294?casa_token=ziKh5Pvf8NIAAAAA:OXKzC4Ck_4f47V0oKB1lBQxdXshK7uISr859oeAI_u3BytSo5ZAeeirEoQjXMBmCkaUKS4GkMNQHEQ},
  number={3},
  pages={28--33},
  year={2022},
  publisher={ACM New York, NY, USA}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@article{baldesi2022charm,
  abbr={Preprint},
  abstract={oday’s radio access networks (RANs) are monolithic
  entities which often operate statically on a given set of parameters
  for the entirety of their operations. To implement realistic and
  effective spectrum sharing policies, RANs will need to seamlessly
  and intelligently change their operational parameters. In stark
  contrast with existing paradigms, the new O-RAN architectures
  for 5G-and-beyond networks (NextG) separate the logic that con-
  trols the RAN from its hardware substrate, allowing unprece-
  dented real-time fine-grained control of RAN components. In
  this context, we propose the Channel-Aware Reactive Mechanism
  (ChARM), a data-driven O-RAN-compliant framework that allows
  (i) sensing the spectrum to infer the presence of interference
  and (ii) reacting in real time by switching the distributed unit
  (DU) and radio unit (RU) operational parameters according to
  a specified spectrum access policy. ChARM is based on neural
  networks operating directly on unprocessed I/Q waveforms to
  determine the current spectrum context. ChARM does not require
  any modification to the existing 3GPP standards. It is designed
  to operate within the O-RAN specifications, and can be used in
  conjunction with other spectrum sharing mechanisms (e.g., LTE-
  U, LTE-LAA or MulteFire). We demonstrate the performance of
  ChARM in the context of spectrum sharing among LTE and Wi-
  Fi in unlicensed bands, where a controller operating over a RAN
  Intelligent Controller (RIC) senses the spectrum and switches
  cell frequency to avoid Wi-Fi. We develop a prototype of ChARM
  using srsRAN, and leverage the Colosseum channel emulator to
  collect a large-scale waveform dataset to train our neural networks
  with. To collect standard-compliant Wi-Fi data, we extended the
  Colosseum testbed using system-on-chip (SoC) boards running
  a modified version of the OpenWiFi architecture. Experimental
  results show that ChARM achieves accuracy of up to 96% on
  Colosseum and 85% on an over-the-air testbed, demonstrating the
  capacity of ChARM to exploit the considered spectrum channels},

  title={ChARM: NextG Spectrum Sharing Through Data-Driven Real-Time O-RAN Dynamic Control},
  author={Baldesi, Luca and Restuccia, Francesco and Melodia, Tommaso},
  journal={arXiv preprint arXiv:2201.06326},
  html={https://arxiv.org/abs/2201.06326},
  year={2022}
}

@article{callegaro2022smartdet,
  abbr={Preprint},
  abstract={Mobile devices increasingly rely on object detec-
  tion (OD) through deep neural networks (DNNs) to perform
  critical tasks. Due to their high complexity, the execution of
  these DNNs requires excessive time and energy. Low-complexity
  object tracking (OT) can be used with OD, where the latter is
  periodically applied to generate “fresh” references for tracking.
  However, the frames processed with OD incur large delays,
  which may make the reference outdated and degrade tracking
  quality. Herein, we propose to use edge computing in this
  context, and establish parallel OT (at the mobile device) and
  OD (at the edge server) processes that are resilient to large OD
  latency. We propose Katch-Up, a novel tracking mechanism that
  improves the system resilience to excessive OD delay. However,
  while Katch-Up significantly improves performance, it also
  increases the computing load of the mobile device. Hence, we
  design SmartDet, a low-complexity controller based on deep
  reinforcement learning (DRL) that learns controlling the trade-
  off between resource utilization and OD performance. SmartDet
  takes as input context-related information related to the current
  video content and the current network conditions to optimize
  frequency and type of OD offloading, as well as Katch-Up
  utilization. We extensively evaluate SmartDet on a real-world
  testbed composed of a JetSon Nano as mobile device and a
  GTX 980 Ti as edge server, connected through a Wi-Fi link.
  Experimental results show that SmartDet achieves an optimal
  balance between tracking performance – mean Average Recall
  (mAR) and resource usage. With respect to a baseline with full
  Katch-Up usage and maximum channel usage, we still increase
  mAR by 4% while using 50% less of the channel and 30% power
  resources associated with Katch-Up. With respect to a fixed
  strategy using minimal resources, we increase mAR by 20% while
  using Katch-Up on 1/3 of the frames.},

  title={SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection},
  author={Callegaro, Davide and Restuccia, Francesco and Levorato, Marco},
  journal={arXiv preprint arXiv:2201.04235},
  html={https://arxiv.org/abs/2201.04235},
  year={2022}
}



@article{matsubara2022bottlefit,
  abbr={Preprint},
  abstract={Although mission-critical applications require the
  use of deep neural networks (DNNs), their continuous execution
  at mobile devices results in a significant increase in energy con-
  sumption. While edge offloading can decrease energy consump-
  tion, erratic patterns in channel quality, network and edge server
  load can lead to severe disruption of the system’s key operations.
  An alternative approach, called split computing, generates com-
  pressed representations within the model (called “bottlenecks”),
  to reduce bandwidth usage and energy consumption. Prior work
  has proposed approaches that introduce additional layers, to the
  detriment of energy consumption and latency. For this reason,
  we propose a new framework called BottleFit, which, in
  addition to targeted DNN architecture modifications, includes
  a novel training strategy to achieve high accuracy even with
  strong compression rates. We apply BottleFit on cutting-edge
  DNN models in image classification, and show that BottleFit
  achieves 77.1% data compression with up to 0.6% accuracy loss
  on ImageNet dataset, while state of the art such as SPINN loses
  up to 6% in accuracy. We experimentally measure the power
  consumption and latency of an image classification application
  running on an NVIDIA Jetson Nano board (GPU-based) and
  a Raspberry PI board (GPU-less). We show that BottleFit
  decreases power consumption and latency respectively by up to
  49% and 89% with respect to (w.r.t.) local computing and by 37%
  and 55% w.r.t. edge offloading. We also compare BottleFit
  with state-of-the-art autoencoders-based approaches, and show
  that (i) BottleFit reduces power consumption and execution
  time respectively by up to 54% and 44% on the Jetson and 40%
  and 62% on Raspberry PI; (ii) the size of the head model executed
  on the mobile device is 83 times smaller. The code repository will
  be published for full reproducibility of the results.},

  title={BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing},
  author={Matsubara, Yoshitomo and Callegaro, Davide and Singh, Sameer and Levorato, Marco and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.02693},
  html={https://arxiv.org/abs/2201.02693},
  year={2022}
}

@article{bahadori2022rewis,
  abbr={Preprint },
  abstract={Thanks to the ubiquitousness of Wi-Fi access points
  and devices, Wi-Fi sensing enables transformative applications
  in remote health care, home/office security, and surveillance, just
  to name a few. Existing work has explored the usage of machine
  learning (ML) on channel state information (CSI) computed from
  Wi-Fi packets to classify events of interest. However, most of
  these algorithms require a significant amount of data collection,
  as well as extensive computational power for additional CSI
  feature extraction. Moreover, the majority of these models suffer
  from poor accuracy when tested in a new/untrained environment.
  In this paper, we propose ReWiS, a novel framework for
  robust and environment-independent Wi-Fi sensing. The key
  innovation of ReWiS is to leverage few-shot learning (FSL) as
  the inference engine, which (i) reduces the need for extensive
  data collection and application-specific feature extraction; (ii)
  can rapidly generalize to new tasks by leveraging only a few
  new samples. Moreover, ReWiS leverages multi-antenna, multi-
  receiver diversity, as well as fine-grained frequency resolution,
  to improve the overall robustness of the algorithms. Finally, we
  propose a technique based on singular value decomposition (SVD)
  to make the FSL input constant irrespective of the number of
  receiver antennas. We prototype ReWiS using off-the-shelf Wi-
  Fi equipment and showcase its performance by considering a
  compelling use case of human activity recognition. Thus, we
  perform an extensive data collection campaign in three different
  propagation environments with two human subjects. We evaluate
  the impact of each diversity component on the performance
  and compare ReWiS with a traditional convolutional neural
  network (CNN) approach. Experimental results show that ReWiS
  improves the performance by about 40% with respect to exist-
  ing single-antenna low-resolution approaches. Moreover, when
  compared to a CNN-based approach, ReWiS shows 35% more
  accuracy and less than 10% drop in accuracy when tested in
  different environments, while the CNN drops by more than 45%.
  To allow reproducibility of our results and to address the current
  dearth of Wi-Fi sensing datasets, we pledge to release our 60 GB
  dataset and the entire code repository to the community.},
  title={ReWiS: Reliable Wi-Fi Sensing Through Few-Shot Multi-Antenna Multi-Receiver CSI Learning},
  author={Bahadori, Niloofar and Ashdown, Jonathan and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.00869},
  html={https://arxiv.org/abs/2201.00869},
  year={2022}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vegni2021series,
  abbr={Journal},
  abstract={In late 2019, a new virus was discovered, namely SARS-CoV-2. This strain causes severe acute respiratory syndrome coronavirus 2, defined as COVID-19. The disease soon spread all over the world, thus becoming a pandemic. It has been almost two years since worldwide restrictions on our lives started, and the traditional way people live and work has completely changed.},
  title={Series Editorial: Networking Technologies to Combat the COVID-19 Pandemic},
  author={Vegni, Anna Maria and Loscr{\`\i}, Valeria and Restuccia, Francesco and Yang, De-Nian},
  journal={IEEE Communications Magazine},
  volume={59},
  number={9},
  pages={14--15},
  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566516},
  year={2021},
  publisher={IEEE}
}


@article{restuccia2021deepfir,
  abbr={Journal},
  abstract={Deep learning can be used to classify waveform characteristics ( e.g. , modulation) with accuracy levels that are hardly attainable with traditional techniques. Recent research has demonstrated that one of the most crucial challenges in wireless deep learning is to counteract the channel action, which may significantly alter the waveform features. The problem is further exacerbated by the fact that deep learning algorithms are hardly re-trainable in real time due to their sheer size. This paper proposes DeepFIR , a framework to counteract the channel action in wireless deep learning algorithms without retraining the underlying deep learning model . The key intuition is that through the application of a carefully-optimized digital finite input response filter (FIR) at the transmitter’s side, we can apply tiny modifications to the waveform to strengthen its features according to the current channel conditions. We mathematically formulate the Waveform Optimization Problem (WOP) as the problem of finding the optimum FIR to be used on a waveform to improve the classifier’s accuracy. We also propose a data-driven methodology to train the FIRs directly with dataset inputs. We extensively evaluate DeepFIR on an experimental testbed of 20 software-defined radios, as well as on two datasets made up by 500 ADS-B devices and by 500 WiFi devices and a 24-class modulation dataset. Experimental results show that our approach (i) increases the accuracy of the radio fingerprinting models by about 35%, 50% and 58%; (ii) decreases an adversary’s accuracy by about 54% when trying to imitate other device’s fingerprints by using their filters; (iii) achieves 27% improvement over the state of the art on a 100-device dataset; (iv) increases by 2× the accuracy of the modulation dataset.},

  title={DeepFIR: Channel-Robust Physical-Layer Deep Learning Through Adaptive Waveform Filtering},
  author={Restuccia, Francesco and D’Oro, Salvatore and Al-Shawabka, Amani and Rendon, Bruno Costa and Ioannidis, Stratis and Melodia, Tommaso},
  journal={IEEE Transactions on Wireless Communications},
  volume={20},
  number={12},
  pages={8054--8066},
  html={https://ieeexplore.ieee.org/abstract/document/9470953},
  year={2021},
  publisher={IEEE}
}



@article{d2021coordinated,
  abbr={Journal},

  abstract={Radio access network (RAN) slicing is a virtualization technology that partitions radio resources into multiple autonomous virtual networks. Since RAN slicing can be tailored to provide diverse performance requirements, it will be pivotal to achieve the high-throughput and low-latency communications that next-generation (5G) systems have long yearned for. To this end, effective RAN slicing algorithms must (i) partition radio resources so as to leverage coordination among multiple base stations and thus boost network throughput; and (ii) reduce interference across different slices to guarantee slice isolation and avoid performance degradation. The ultimate goal of this paper is to design RAN slicing algorithms that address the above two requirements. First, we show that the RAN slicing problem can be formulated as a 0-1 Quadratic Programming problem, and we prove its NP-hardness. Second, we propose an optimal solution for small-scale 5G network deployments, and we present three approximation algorithms to make the optimization problem tractable when the network size increases. We first analyze the performance of our algorithms through simulations, and then demonstrate their performance through experiments on a standard-compliant LTE testbed with 2 base stations and 6 smartphones. Our results show that not only do our algorithms efficiently partition RAN resources, but also improve network throughput by 27% and increase by 2× the signal-to-interference-plus-noise ratio.},


  title={Coordinated 5G network slicing: How constructive interference can boost network throughput},
  author={D’Oro, Salvatore and Bonati, Leonardo and Restuccia, Francesco and Melodia, Tommaso},
  journal={IEEE/ACM Transactions on Networking},
  volume={29},
  number={4},
  pages={1881--1894},
  year={2021},

  html={https://ieeexplore.ieee.org/abstract/document/9411723},
  publisher={IEEE}
}

@article{guida2021implantable,
  abbr={Journal},

  abstract={The promise of real-time detection and response to life-crippling diseases brought by the Implantable Internet of Medical Things (IIoMT) has recently spurred substantial advances in implantable technologies. Yet, existing medical devices do not provide at once the miniaturized end-to-end body monitoring, wireless communication and remote powering capabilities to implement IIoMT applications. This paper fills the existing research gap by presenting U-Verse, the first FDA-compliant rechargeable IIoMT platform packing sensing, computation, communication, and recharging circuits into a penny-scale platform. Extensive experimental evaluation indicates that U-Verse (i) can be wirelessly recharged and can store energy several orders of magnitude more than state-of-theart capacity in tens of minutes; (ii) with one single charge, it can operate from few hours to several days. Finally, U-Verse is demonstrated through (i) a closed-loop application that sends data via ultrasounds through real porcine meat; and (ii) a real-time reconfigurable pacemaker.},

  title={The Implantable Internet of Medical Things: Toward Lifelong Remote Monitoring and Treatment of Chronic Diseases},
  author={Guida, Raffaele and Dave, Neil and Restuccia, Francesco and Demirors, Emrecan and Melodia, Tommaso},
  journal={GetMobile: Mobile Computing and Communications},
  volume={24},
  number={3},
  pages={20--25},
  year={2021},
  html={https://dl.acm.org/doi/abs/10.1145/3447853.3447861},
  publisher={ACM New York, NY, USA}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{tehrani2021federated,
  abbr={Conference},
  abstract={Next Generation (NextG) networks are expected to
  support demanding tactile internet applications such as aug-
  mented reality and connected autonomous vehicles. Whereas
  recent innovations bring the promise of larger link capacity,
  their sensitivity to the environment and erratic performance
  defy traditional model-based control rationales. Zero-touch data-
  driven approaches can improve the ability of the network
  to adapt to the current operating conditions. Tools such as
  reinforcement learning (RL) algorithms can build optimal control
  policy solely based on a history of observations. Specifically,
  deep RL (DRL), which uses a deep neural network (DNN)
  as a predictor, has been shown to achieve good performance
  even in complex environments and with high dimensional inputs.
  However, the training of DRL models require a large amount of
  data, which may limit its adaptability to ever-evolving statistics
  of the underlying environment. Moreover, wireless networks
  are inherently distributed systems, where centralized DRL ap-
  proaches would require excessive data exchange, while fully
  distributed approaches may result in slower convergence rates
  and performance degradation. In this paper, to address these
  challenges, we propose a federated learning (FL) approach to
  DRL, which we refer to federated DRL (F-DRL), where base
  stations (BS) collaboratively train the embedded DNN by only
  sharing models’ weights rather than training data. We evaluate
  two distinct versions of F-DRL, value and policy based, and show
  the superior performance they achieve compared to distributed
  and centralized DRL},

  title={Federated Deep Reinforcement Learning for the Distributed Control of NextG Wireless Networks},
  author={Tehrani, Peyman and Restuccia, Francesco and Levorato, Marco},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={248--253},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677132},
  organization={IEEE}
}

@inproceedings{bonati2021colosseum,
  abbr={Conference},
  abstract={Colosseum is an open-access and publicly-available
  large-scale wireless testbed for experimental research via virtu-
  alized and softwarized waveforms and protocol stacks on a fully
  programmable, “white-box” platform. Through 256 state-of-the-
  art software-defined radios and a massive channel emulator core,
  Colosseum can model virtually any scenario, enabling the de-
  sign, development and testing of solutions at scale in a variety
  of deployments and channel conditions. These Colosseum radio-
  frequency scenarios are reproduced through high-fidelity FPGA-
  based emulation with finite-impulse response filters. Filters model
  the taps of desired wireless channels and apply them to the signals
  generated by the radio nodes, faithfully mimicking the conditions
  of real-world wireless environments. In this paper, we introduce
  Colosseum as a testbed that is for the first time open to the research
  community. We describe the architecture of Colosseum and its
  experimentation and emulation capabilities. We then demonstrate
  the effectiveness of Colosseum for experimental research at scale
  through exemplary use cases including prevailing wireless tech-
  nologies (e.g., cellular and Wi-Fi) in spectrum sharing and un-
  manned aerial vehicle scenarios. A roadmap for Colosseum future
  updates concludes the paper.},

  title={Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation},
  author={Bonati, Leonardo and Johari, Pedram and Polese, Michele and D’Oro, Salvatore and Mohanti, Subhramoy and Tehrani-Moayyed, Miead and Villa, Davide and Shrivastava, Shweta and Tassie, Chinenye and Yoder, Kurt and others},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={105--113},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677430},
  organization={IEEE}
}


@inproceedings{al2021deeplora,
  abbr={Conference},
  abstract={

  The Long Range (LoRa) protocol for low-power wide-area networks (LPWANs) is a strong candidate to enable the massive roll-out of the Internet of Things (IoT) because of its low cost, impressive sensitivity (-137dBm), and massive scalability potential. As tens of thousands of tiny LoRa devices are deployed over large geographic areas, a key component to the success of LoRa will be the development of reliable and robust authentication mechanisms. To this end, Radio Frequency Fingerprinting (RFFP) through deep learning (DL) has been heralded as an effective zero-power supplement or alternative to energy-hungry cryptography. Existing work on LoRa RFFP has mostly focused on small-scale testbeds and low-dimensional learning techniques; however, many challenges remain. Key among them are authentication techniques robust to a wide variety of channel variations over time and supporting a vast population of devices.

  In this work, we advance the state of the art by presenting (i) the first massive experimental evaluation of DL RFFP and (ii) new data augmentation techniques for LoRa designed to counter the degradation introduced by the wireless channel. Specifically, we collected and publicly shared more than 1TB of waveform data from 100 bit-similar devices (with identical manufacturing processes) over different deployment scenarios (outdoor vs. indoor) and spanning several days. We train and test diverse DL models (convolutional and recurrent neural networks) using either preamble or payload data slices. We compare three different representations of the received signal: (i) IQ, (ii) amplitude-phase, and (iii) spectrogram. Finally, we propose a novel data augmentation technique called DeepLoRa to enhance the LoRa RFFP performance. Results show that (i) training the CNN models with IQ representation is not always the best combo in fingerprinting LoRa radios; training CNNs and RNN-LSTMs with amplitude-phase and spectrogram representations may increase the fingerprinting performance in small and medium-scale testbeds; (ii) using only payload data in the fingerprinting process outperforms preamble only data, and (iii) DeepLoRa data augmentation technique improves the classification accuracy from 19% to 36% in the RFFP challenging case of training on data collected on a different day than the testing data. Moreover, DeepLoRa raises the accuracy from 82% to 91% when training and testing 100 devices with data collected on the same day.},
  title={DeepLoRa: Fingerprinting LoRa Devices at Scale Through Deep Learning and Data Augmentation},

  author={Al-Shawabka, Amani and Pietraski, Philip and Pattar, Sudhir B and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={251--260},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467054},
  year={2021}
}



@inproceedings{piva2021tags,
  abbr={Conference},
  abstract={Millions of RFID tags are pervasively used all around the globe to inexpensively identify a wide variety of everyday-use objects. One of the key issues of RFID is that tags cannot use energy-hungry cryptography, and thus can be easily cloned. For this reason, radio fingerprinting (RFP) is a compelling approach that leverages the unique imperfections in the tag's wireless circuitry to achieve large-scale RFID clone detection. Recent work, however, has unveiled that time-varying channel conditions can significantly decrease the accuracy of the RFP process. Prior art in RFID identification does not consider this critical aspect, and instead focuses on custom-tailored feature extraction techniques and data collection with static channel conditions. For this reason, we propose the first large-scale investigation into RFP of RFID tags with dynamic channel conditions. Specifically, we perform a massive data collection campaign on a testbed composed by 200 off-the-shelf identical RFID tags and a software-defined radio (SDR) tag reader. We collect data with different tag-reader distances in an over-the-air configuration. To emulate implanted RFID tags, we also collect data with two different kinds of porcine meat inserted between the tag and the reader. We use this rich dataset to train and test several convolutional neural network (CNN)-based classifiers in a variety of channel conditions. Our investigation reveals that training and testing on different channel conditions drastically degrades the classifier's accuracy. For this reason, we propose a novel training framework based on federated machine learning (FML) and data augmentation (DAG) to boost the accuracy. Extensive experimental results indicate that (i) our FML approach improves accuracy by up to 48%; (ii) our DAG approach improves the FML performance by up to 19% and the single-dataset performance by 31%. To the best of our knowledge, this is the first paper experimentally demonstrating the efficacy of FML and DAG on a large device population. To allow full replicability, we are sharing with the research community our fully-labeled 200-GB RFID waveform dataset, as well as the entirety of our code and trained models, concurrently with our submission.},

  title={The Tags Are Alright: Robust Large-Scale RFID Clone Detection Through Federated Data-Augmented Radio Fingerprinting},
  author={Piva, Mauro and Maselli, Gaia and Restuccia, Francesco},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467033},
  pages={41--50},
  year={2021}
}


@inproceedings{polese2021deepbeam,
  abbr={Conference},
  abstract={Highly directional millimeter wave (mmWave) radios need to perform beam management to establish and maintain reliable links. To achieve this objective, existing solutions mostly rely on explicit coordination between the transmitter (TX) and the receiver (RX), which significantly reduces the airtime available for communication and further complicates the network protocol design. This paper advances the state of the art by presenting DeepBeam, a framework for beam management that does not require pilot sequences from the TX, nor any beam sweeping or synchronization from the RX. This is achieved by inferring (i) the Angle of Arrival (AoA) of the beam and (ii) the actual beam being used by the transmitter through waveform-level deep learning on ongoing transmissions between the TX to other receivers. In this way, the RX can associate Signal-to-Noise-Ratio (SNR) levels to beams without explicit coordination with the TX. This is possible because different beam patterns introduce different "impairments" to the waveform, which can be subsequently learned by a convolutional neural network (CNN). To demonstrate the generality of DeepBeam, we conduct an extensive experimental data collection campaign where we collect more than 4 TB of mmWave waveforms with (i) 4 phased array antennas at 60.48 GHz, (ii) 2 codebooks containing 24 one-dimensional beams and 12 two-dimensional beams; (iii) 3 receiver gains; (iv) 3 different AoAs; (v) multiple TX and RX locations. Moreover, we collect waveform data with two custom-designed mmWave software-defined radios with fully-digital beamforming architectures at 58 GHz. We also implement our learning models in FPGA to evaluate latency performance. Results show that DeepBeam (i) achieves accuracy of up to 96%, 84% and 77% with a 5-beam, 12-beam and 24-beam codebook, respectively; (ii) reduces latency by up to 7x with respect to the 5G NR initial beam sweep in a default configuration and with a 12-beam codebook. The waveform dataset and the full DeepBeam code repository are publicly available.},

  title={DeepBeam: Deep Waveform Learning for Coordination-Free Beam Management in mmWave Networks},
  author={Polese, Michele and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467035},
  pages={61--70},
  year={2021}
}


@inproceedings{ghiro2021blockchain,
  abbr={Conference},
  abstract={The term blockchain is used for disparate projects, ranging from cryptocurrencies to applications for the Internet of Things (IoT). The concept of blockchain appears therefore blurred, as the same technology cannot empower applications with extremely different requirements, levels of security and performance. This position paper elaborates on the theory of distributed systems to advance a clear definition of blockchain allowing us to clarify its possible role in the IoT. The definition binds together three elements that, as a whole, delineate those unique features that distinguish the blockchain from other distributed ledger technologies: immutability, transparency and anonymity. We note that immutability-which is imperative for securing blockchains-imposes remarkable resource consumption. Moreover, while transparency demands no confidentiality, anonymity enhances privacy but prevents user identification. As such, we raise the concern that these blockchain features clash with the requirements of most IoT applications where devices are power-constrained, data needs to be kept confidential, and users to be clearly identifiable. We consequently downplay the role of the blockchain for the IoT: this can act as a ledger external to the IoT architecture, invoked as seldom as possible and only to record the aggregate results of myriads of local (IoT) transactions that are most of the time performed off-chain to meet performance and scalability requirements.},
  
  title={A Blockchain Definition to Clarify its Role for the Internet of Things},

  author={Ghiro, Lorenzo and Restuccia, Francesco and D'Oro, Salvatore and Basagni, Stefano and Melodia, Tommaso and Maccari, Leonardo and Cigno, Renato Lo},
  booktitle={2021 19th Mediterranean Communication and Computer Networking Conference (MedComNet)},
  pages={1--8},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9501280},
  organization={IEEE}
}



@inproceedings{uvaydov2021deepsense,
  abbr={Conference},

  abstract={Spectrum sharing will be a key technology to tackle
  spectrum scarcity in the sub-6 GHz bands. To fairly access the
  shared bandwidth, wireless users will necessarily need to quickly
  sense large portions of spectrum and opportunistically access
  unutilized bands. The key unaddressed challenges of spectrum
  sensing are that (i) it has to be performed with extremely low
  latency over large bandwidths to detect tiny spectrum holes and
  to guarantee strict real-time digital signal processing (DSP) con-
  straints; (ii) its underlying algorithms need to be extremely accu-
  rate, and flexible enough to work with different wireless bands
  and protocols to find application in real-world settings. To the
  best of our knowledge, the literature lacks spectrum sensing tech-
  niques able to accomplish both requirements. In this paper, we
  propose DeepSense, a software/hardware framework for real-time
  wideband spectrum sensing that relies on real-time deep learn-
  ing tightly integrated into the transceiver’s baseband processing
  logic to detect and exploit unutilized spectrum bands. DeepSense
  uses a convolutional neural network (CNN) implemented in the
  wireless platform’s hardware fabric to analyze a small portion of
  the unprocessed baseband waveform to automatically extract the
  maximum amount of information with the least amount of I/Q
  samples. We extensively validate the accuracy, latency and gen-
  erality performance of DeepSense with (i) a 400 GB dataset con-
  taining hundreds of thousands of WiFi transmissions collected “in
  the wild” with different Signal-to-Noise-Ratio (SNR) conditions
  and over different days; (ii) a dataset of transmissions collected
  using our own software-defined radio testbed; and (iii) a synthetic
  dataset of LTE transmissions under controlled SNR conditions.
  We also measure the real-time latency of the CNNs trained on
  the three datasets with an FPGA implementation, and compare
  our approach with a fixed energy threshold mechanism. Results
  show that our learning-based approach can deliver a precision
  and recall of 98% and 97% respectively and a latency as low
  as 0.61ms. For reproducibility and benchmarking purposes, we
  pledge to share the code and the datasets used in this paper to
  the community.},

  title={Deepsense: Fast wideband spectrum sensing through real-time in-the-loop deep learning},
  author={Uvaydov, Daniel and D’Oro, Salvatore and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},

  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9488764},
  year={2021},
  organization={IEEE}
}

@inproceedings{d2021can,
  abbr={Conference},
  abstract= {Due to the sheer scale of the Internet of Things (IoT)
  and 5G, the wireless spectrum is becoming severely congested.
  For this reason, wireless devices will need to continuously adapt
  to current spectrum conditions by changing their communication
  parameters in real-time. Therefore, wireless signal classification
  (WSC) will become a compelling necessity to decode fast-changing
  signals from dynamic transmitters. Thanks to its capability of
  classifying complex phenomena without explicit mathematical
  modeling, deep learning (DL) has been demonstrated to be a key
  enabler of WSC. Although DL can achieve a very high accuracy
  under certain conditions, recent research has unveiled that the
  wireless channel can disrupt the features learned by the DL model
  during training, thus drastically reducing the classification per-
  formance in real-world live settings. Since retraining classifiers
  is cumbersome after deployment, existing work has leveraged
  the usage of carefully-tailored Finite Impulse Response (FIR)
  filters that, when applied at the transmitter’s side, can restore
  the features that are lost because of the the channel actions,
  i.e., waveform synthesis. However, these approaches compute FIRs
  using offline optimization strategies, which limits their efficacy
  in highly-dynamic channel settings. In this paper, we improve
  the state of the art by proposing Chares, a Deep Reinforcement
  Learning (DRL)-based framework for channel-resilient adaptive
  waveform synthesis. Chares adapts to new and unseen channel
  conditions by optimally computing through DRL the FIRs in real
  time. Chares is a DRL agent whose architecture is based upon
  the Twin Delayed Deep Deterministic Policy Gradients (TD3),
  which requires minimal feedback from the receiver and explores a
  continuous action space for best performance. Chares has been ex-
  tensively evaluated on two well-known datasets with an extensive
  number of channels. We have also evaluated the real-time latency
  of Chares with an implementation on field-programmable gate
  array (FPGA). Results show that Chares increases the accuracy
  up to 4.1x when no waveform synthesis is performed, by 1.9x with
  respect to existing work, and can compute new actions within 41μs.},

  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9488865},
  title={Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for Resilient Wireless Signal Classification},
  author={D’Oro, Salvatore and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2021},
  organization={IEEE}
}


@inproceedings{bonati2021stealte,
  abbr={Conference},
  abstract={Fifth-generation (5G) systems will extensively employ radio access network (RAN) softwarization. This key innovation enables the instantiation of "virtual cellular networks" running on different slices of the shared physical infrastructure. In this paper, we propose the concept of Private Cellular Connectivity as a Service (PCCaaS), where infrastructure providers deploy covert network slices known only to a subset of users. We then present SteaLTE as the first realization of a PCCaaS-enabling system for cellular networks. At its core, SteaLTE utilizes wireless steganography to disguise data as noise to adversarial receivers. Differently from previous work, however, it takes a full-stack approach to steganography, contributing an LTE-compliant stegano-graphic protocol stack for PCCaaS-based communications, and packet schedulers and operations to embed covert data streams on top of traditional cellular traffic (primary traffic). SteaLTE balances undetectability and performance by mimicking channel impairments so that covert data waveforms are almost indistinguishable from noise. We evaluate the performance of SteaLTE on an indoor LTE-compliant testbed under different traffic profiles, distance and mobility patterns. We further test it on the outdoor PAWR POWDER platform over long-range cellular links. Results show that in most experiments SteaLTE imposes little loss of primary traffic throughput in presence of covert data transmissions (<; 6%), making it suitable for undetectable PCCaaS networking.},

  title={SteaLTE: Private 5G Cellular Connectivity as a Service with Full-stack Wireless Steganography},
  author={Bonati, Leonardo and D’Oro, Salvatore and Restuccia, Francesco and Basagni, Stefano and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9488889},
  organization={IEEE}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





@article{restuccia2021ieee,
  abbr={Preprint},

  abstract={Wi-Fi is among the most successful wireless technologies ever invented. As Wi-Fi becomes more and more present in public and private spaces, it becomes natural to leverage its ubiquitousness to implement groundbreaking wireless sensing applications such as human presence detection, activity recognition, and object tracking, just to name a few. This paper reports ongoing efforts by the IEEE 802.11bf Task Group (TGbf), which is defining the appropriate modifications to existing Wi-Fi standards to enhance sensing capabilities through 802.11-compliant waveforms. We summarize objectives and timeline of TGbf, and discuss some of the most interesting proposed technical features discussed so far. We also introduce a roadmap of research challenges pertaining to Wi-Fi sensing and its integration with future Wi-Fi technologies and emerging spectrum bands, hoping to elicit further activities by both the research community and TGbf. },

  title={IEEE 802.11 bf: Toward ubiquitous Wi-Fi sensing},
  author={Restuccia, Francesco},
  html={https://arxiv.org/abs/2103.14918},
  journal={arXiv preprint arXiv:2103.14918},
  year={2021}
}



@article{matsubara2021split,
  abbr={Preprint},

  abstract={Mobile devices such as smartphones and autonomous vehicles increasingly rely on deep neural networks (DNNs) to execute complex inference tasks such as image classification and speech recognition, among others. However, continuously executing the entire DNN on the mobile device can quickly deplete its battery. Although task offloading to cloud/edge servers may decrease the mobile device's computational burden, erratic patterns in channel quality, network, and edge server load can lead to a significant delay in task execution. Recently, approaches based on split computing (SC) have been proposed, where the DNN is split into a head and a tail model, executed respectively on the mobile device and on the edge server. Ultimately, this may reduce bandwidth usage as well as energy consumption. Another approach, called early exiting (EE), trains models to present multiple "exits" earlier in the architecture, each providing increasingly higher target accuracy. Therefore, the trade-off between accuracy and delay can be tuned according to the current conditions or application demands. In this paper, we provide a comprehensive survey of the state of the art in SC and EE strategies by presenting a comparison of the most relevant approaches. We conclude the paper by providing a set of compelling research challenges.},

  title={Split computing and early exiting for deep learning applications: Survey and research challenges},
  author={Matsubara, Yoshitomo and Levorato, Marco and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2103.04505},
  html={https://arxiv.org/abs/2103.04505},
  year={2021}
}


@article{ghiro2021blockchain,
  abbr={Preprint },
  
  abstract={The use of the term blockchain is documented for disparate projects, from cryptocurrencies to applications for the Internet of Things (IoT), and many more. The concept of blockchain appears therefore blurred, as it is hard to believe that the same technology can empower applications that have extremely different requirements and exhibit dissimilar performance and security. This position paper elaborates on the theory of distributed systems to advance a clear definition of blockchain that allows us to clarify its role in the IoT. This definition inextricably binds together three elements that, as a whole, provide the blockchain with those unique features that distinguish it from other distributed ledger technologies: immutability, transparency and anonimity. We note however that immutability comes at the expense of remarkable resource consumption, transparency demands no confidentiality and anonymity prevents user identification and registration. This is in stark contrast to the requirements of most IoT applications that are made up of resource constrained devices, whose data need to be kept confidential and users to be clearly known. Building on the proposed definition, we derive new guidelines for selecting the proper distributed ledger technology depending on application requirements and trust models, identifying common pitfalls leading to improper applications of the blockchain. We finally indicate a feasible role of the blockchain for the IoT: myriads of local, IoT transactions can be aggregated off-chain and then be successfully recorded on an external blockchain as a means of public accountability when required.},

  title={What is a Blockchain? A Definition to Clarify the Role of the Blockchain in the Internet of Things},
  author={Ghiro, Lorenzo and Restuccia, Francesco and D'Oro, Salvatore and Basagni, Stefano and Melodia, Tommaso and Maccari, Leonardo and Cigno, Renato Lo},
  journal={arXiv preprint arXiv:2102.03750},

  html={https://arxiv.org/abs/2102.03750},
  year={2021}
}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2020   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2020   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2020   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@article{al2020massive,
  abbr={Journal},

  abstract={Recent research has proved the effectiveness of neural networks (NNs) in “fingerprinting” (i.e., identifying) wireless radios, by determining the hardware impairments emitted from the transmitter during the waveform transmission process. The artificial neurons of the NN layers are employed to identify and track the radios’ unique impairments by training a large amount of raw data released from these radios. Today, the radio fingerprinting field lacks such a large-scale waveform database that can provide a standard benchmark for researchers working on this field. In this paper, we publicly share 2TB of IEEE 802.11 a/g (WiFi) data obtained from 20 bit-similar Software-Defined-Radios (SDRs).},

  title={Massive-Scale I/Q Datasets for WiFi Radio Fingerprinting},
  author={Al-Shawabka, Amani and Restuccia, Francesco and D’Oro, Salvatore and Melodia, Tommaso},
  journal={Computer Networks},
  volume={182},
  pages={107566},
  html={https://www.sciencedirect.com/science/article/pii/S1389128620312123},
  year={2020},
  publisher={Elsevier}
}

@article{bertizzolo2020arena,
  abbr={Journal},

  abstract={Arena is an open-access wireless testing platform based on a grid of antennas mounted on the ceiling of a large
  office-space environment. Each antenna is connected to programmable software-defined radios (SDR) enabling
  sub-6 GHz 5G-and-beyond spectrum research. With 12 computational servers, 24 SDRs synchronized at the
  symbol level, and a total of 64 antennas, Arena provides the computational power and the scale to foster new
  technology development in some of the most crowded spectrum bands. Arena is based on a three-tier design,
  where the servers and the SDRs are housed in a double rack in a dedicated room, while the antennas are hung off
  the ceiling of a 2240 square feet office space and cabled to the radios through 100 ft-long cables. This ensures a
  reconfigurable, scalable, and repeatable real-time experimental evaluation in a real wireless indoor environment.
  In this paper, we introduce the architecture, capabilities, and system design choices of Arena, and provides
  details of the software and hardware implementation of various testbed components. Furthermore, we describe
  key capabilities by providing examples of published work that employed Arena for applications as diverse as
  synchronized MIMO transmission schemes, multi-hop ad hoc networking, multi-cell 5G networks, AI-powered
  Radio-Frequency fingerprinting, secure wireless communications, and spectrum sensing for cognitive radio.},

  title={Arena: A 64-antenna SDR-based ceiling grid testing platform for sub-6 GHz 5G-and-Beyond radio spectrum research},
  author={Bertizzolo, Lorenzo and Bonati, Leonardo and Demirors, Emrecan and Al-Shawabka, Amani and D’Oro, Salvatore and Restuccia, Francesco and Melodia, Tommaso},
  journal={Computer Networks},
  volume={181},
  pages={107436},
  year={2020},
  html={https://www.sciencedirect.com/science/article/pii/S1389128620311257},
  publisher={Elsevier}
}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




@inproceedings{moayyed2020comparative,
  abbr={Conference},

  abstract={The extremely high data rates provided by communications in the millimeter-length (mmWave) frequency bands can help address the unprecedented demands of next-generation wireless communications. However, atmospheric attenuation and high propagation loss severely limit the coverage of mmWave networks. To overcome these challenges, multi-input-multi-output (MIMO) provides beamforming capabilities and high-gain steerable antennas to expand communication coverage at mmWave frequencies. The main contribution of this paper is the performance evaluation of mmWave communications on top of the recently released NR standard for 5G cellular networks. Furthermore, we compare the performance of NR with the 4G long-term evolution (LTE) standard on a highly realistic campus environment. We consider physical layer constraints such as transmit power, ambient noise, receiver noise figure, and practical antenna gain in both cases, and examine bitrate and area coverage as the criteria to benchmark the performance. We also show the impact of MIMO technology to improve the performance of the 5G NR cellular network. Our evaluation demonstrates that 5G NR provides on average 6.7 times bitrate improvement without remarkable coverage degradation.},

  title={Comparative performance evaluation of mmWave 5G NR and LTE in a campus scenario},
  author={Moayyed, Miead Tehrani and Restuccia, Francesco and Basagni, Stefano},
  booktitle={2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall)},

  html={https://ieeexplore.ieee.org/abstract/document/9348727},
  pages={1--5},
  year={2020},
  organization={IEEE}
}


@inproceedings{maselli2020hyblose,
  abbr={Conference},

  abstract={Although smart environments are a key component of the Internet of Things (IoT), it is also clear that billions connected doors, washing machines, ovens and others will ultimately raise security and privacy concerns. Early work in this area, as well as most of commercial solutions, has adopted a centralized client/server approach, neglecting the multitude of risks that are induced by an unfair control of the server side. This has made the adoption of a decentralized and trust-less framework quintessential to guarantee devices security. Nevertheless, decentralized proposals are hardly applicable due to costs, slowness and privacy issues. In this paper, we make the use of blockchain practical for smart environments by designing HyBloSE, a secure-by-design and lightweight blockchain-based framework, able to run on low-power devices without additional hardware. HyBloSE is built by using Delegated Proof of Authority and a Moving Window Blockchain. We evaluate HyBloSE through a network emulator and real experiments with different Raspberry Pi platforms. Results show that HyBloSE guarantees a higher security level in terms of resiliency to internal and external attacks compared to centralized solutions, with overhead below 0.38s per operation and less than $4 per month for unlimited operations. Furthermore, we show how Proof of Authority is more adapt then Proof of Work in IoT private scenarios.},


  title={HyBloSE: hybrid blockchain for secure-by-design smart environments},
  author={Maselli, Gaia and Piva, Mauro and Restuccia, Francesco},
  booktitle={Proceedings of the 3rd Workshop on Cryptocurrencies and Blockchains for Distributed Systems},
  html={https://dl.acm.org/doi/abs/10.1145/3410699.3413793},

  pages={23--28},
  year={2020}
}



@inproceedings{restuccia2020generalized,
  abbr={Conference},

  abstract={Deep learning techniques can classify spectrum phenomena (e.g., waveform modulation) with accuracy levels that were once thought impossible. Although we have recently seen many advances in this field, extensive work in computer vision has demonstrated that an adversary can "crack" a classifier by designing inputs that "steer" the classifier away from the ground truth. This paper advances the state of the art by proposing a generalized analysis and evaluation of adversarial machine learning (AML) attacks to deep learning systems in the wireless domain. We postulate a series of adversarial attacks, and formulate a Generalized Wireless Adversarial Machine Learning Problem (GWAP) where we analyze the combined effect of the wireless channel and the adversarial waveform on the efficacy of the attacks. We extensively evaluate the performance of our attacks on a state-of-the-art 1,000-device radio fingerprinting dataset, and a 24-class modulation dataset. Results show that our algorithms can decrease the classifiers' accuracy up to 3x while keeping the waveform distortion to a minimum.},


  title={Generalized wireless adversarial deep learning},
  author={Restuccia, Francesco and D'Oro, Salvatore and Al-Shawabka, Amani and Rendon, Bruno Costa and Chowdhury, Kaushik and Ioannidis, Stratis and Melodia, Tommaso},

  html={https://dl.acm.org/doi/abs/10.1145/3395352.3402625},

  booktitle={Proceedings of the 2nd ACM Workshop on Wireless Security and Machine Learning},
  pages={49--54},
  year={2020}
}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{restuccia2020deepfir,

  abbr={Preprint},


  abstract={ Deep learning can be used to classify waveform characteristics (e.g., modulation) with accuracy levels that are hardly attainable with traditional techniques. Recent research has demonstrated that one of the most crucial challenges in wireless deep learning is to counteract the channel action, which may significantly alter the waveform features. The problem is further exacerbated by the fact that deep learning algorithms are hardly re-trainable in real time due to their sheer size. This paper proposes DeepFIR, a framework to counteract the channel action in wireless deep learning algorithms without retraining the underlying deep learning model. The key intuition is that through the application of a carefully-optimized digital finite input response filter (FIR) at the transmitter's side, we can apply tiny modifications to the waveform to strengthen its features according to the current channel conditions. We mathematically formulate the Waveform Optimization Problem (WOP) as the problem of finding the optimum FIR to be used on a waveform to improve the classifier's accuracy. We also propose a data-driven methodology to train the FIRs directly with dataset inputs. We extensively evaluate DeepFIR on a experimental testbed of 20 software-defined radios, as well as on two datasets made up by 500 ADS-B devices and by 500 WiFi devices and a 24-class modulation dataset. Experimental results show that our approach (i) increases the accuracy of the radio fingerprinting models by about 35%, 50% and 58%; (ii) decreases an adversary's accuracy by about 54% when trying to imitate other device's fingerprints by using their filters; (iii) achieves 27% improvement over the state of the art on a 100-device dataset; (iv) increases by 2x the accuracy of the modulation dataset. },


  title={DeepFIR: Addressing the Wireless Channel Action in Physical-Layer Deep Learning},
  author={Restuccia, Francesco and D'Oro, Salvatore and Al-Shawabka, Amani and Rendon, Bruno Costa and Ioannidis, Stratis and Melodia, Tommaso},
  journal={arXiv preprint arXiv:2005.04226},

  html={https://arxiv.org/abs/2005.04226},
  year={2020}
}


