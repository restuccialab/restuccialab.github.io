---
---


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{restuccia2022toward,
  abbr={Journal},
  abstract={Wireless systems such as the Internet of Things (IoT) are changing the way we interact with the cyber and the physical world. As IoT systems become more and more pervasive, it is imperative to design wireless protocols that can effectively and efficiently support IoT devices and operations. On the other hand, today's IoT wireless systems are based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. In this paper, we introduce the new notion of a deep learning-based polymorphic IoT receiver, able to reconfigure its waveform demodulation strategy itself in real time, based on the inferred waveform parameters. Our key innovation is the introduction of a novel embedded deep learning architecture that enables the solution of waveform inference problems, which is then integrated into a generalized hardware/software architecture with radio components and signal processing. Our polymorphic wireless receiver is prototyped on a custom-made software-defined radio platform. We show through extensive over-the-air experiments that the system achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible.},


  title={Toward Polymorphic Internet of Things Receivers Through Real-Time Waveform-Level Deep Learning},
  author={Restuccia, Francesco and Melodia, Tommaso},
  journal={GetMobile: Mobile Computing and Communications},
  volume={25},
  html={https://dl.acm.org/doi/abs/10.1145/3511285.3511294?casa_token=ziKh5Pvf8NIAAAAA:OXKzC4Ck_4f47V0oKB1lBQxdXshK7uISr859oeAI_u3BytSo5ZAeeirEoQjXMBmCkaUKS4GkMNQHEQ},
  number={3},
  pages={28--33},
  year={2022},
  publisher={ACM New York, NY, USA}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@article{baldesi2022charm,
  abbr={Preprint},
  abstract={oday’s radio access networks (RANs) are monolithic
  entities which often operate statically on a given set of parameters
  for the entirety of their operations. To implement realistic and
  effective spectrum sharing policies, RANs will need to seamlessly
  and intelligently change their operational parameters. In stark
  contrast with existing paradigms, the new O-RAN architectures
  for 5G-and-beyond networks (NextG) separate the logic that con-
  trols the RAN from its hardware substrate, allowing unprece-
  dented real-time fine-grained control of RAN components. In
  this context, we propose the Channel-Aware Reactive Mechanism
  (ChARM), a data-driven O-RAN-compliant framework that allows
  (i) sensing the spectrum to infer the presence of interference
  and (ii) reacting in real time by switching the distributed unit
  (DU) and radio unit (RU) operational parameters according to
  a specified spectrum access policy. ChARM is based on neural
  networks operating directly on unprocessed I/Q waveforms to
  determine the current spectrum context. ChARM does not require
  any modification to the existing 3GPP standards. It is designed
  to operate within the O-RAN specifications, and can be used in
  conjunction with other spectrum sharing mechanisms (e.g., LTE-
  U, LTE-LAA or MulteFire). We demonstrate the performance of
  ChARM in the context of spectrum sharing among LTE and Wi-
  Fi in unlicensed bands, where a controller operating over a RAN
  Intelligent Controller (RIC) senses the spectrum and switches
  cell frequency to avoid Wi-Fi. We develop a prototype of ChARM
  using srsRAN, and leverage the Colosseum channel emulator to
  collect a large-scale waveform dataset to train our neural networks
  with. To collect standard-compliant Wi-Fi data, we extended the
  Colosseum testbed using system-on-chip (SoC) boards running
  a modified version of the OpenWiFi architecture. Experimental
  results show that ChARM achieves accuracy of up to 96% on
  Colosseum and 85% on an over-the-air testbed, demonstrating the
  capacity of ChARM to exploit the considered spectrum channels},

  title={ChARM: NextG Spectrum Sharing Through Data-Driven Real-Time O-RAN Dynamic Control},
  author={Baldesi, Luca and Restuccia, Francesco and Melodia, Tommaso},
  journal={arXiv preprint arXiv:2201.06326},
  html={https://arxiv.org/abs/2201.06326},
  year={2022}
}

@article{callegaro2022smartdet,
  abbr={Preprint},
  abstract={Mobile devices increasingly rely on object detec-
  tion (OD) through deep neural networks (DNNs) to perform
  critical tasks. Due to their high complexity, the execution of
  these DNNs requires excessive time and energy. Low-complexity
  object tracking (OT) can be used with OD, where the latter is
  periodically applied to generate “fresh” references for tracking.
  However, the frames processed with OD incur large delays,
  which may make the reference outdated and degrade tracking
  quality. Herein, we propose to use edge computing in this
  context, and establish parallel OT (at the mobile device) and
  OD (at the edge server) processes that are resilient to large OD
  latency. We propose Katch-Up, a novel tracking mechanism that
  improves the system resilience to excessive OD delay. However,
  while Katch-Up significantly improves performance, it also
  increases the computing load of the mobile device. Hence, we
  design SmartDet, a low-complexity controller based on deep
  reinforcement learning (DRL) that learns controlling the trade-
  off between resource utilization and OD performance. SmartDet
  takes as input context-related information related to the current
  video content and the current network conditions to optimize
  frequency and type of OD offloading, as well as Katch-Up
  utilization. We extensively evaluate SmartDet on a real-world
  testbed composed of a JetSon Nano as mobile device and a
  GTX 980 Ti as edge server, connected through a Wi-Fi link.
  Experimental results show that SmartDet achieves an optimal
  balance between tracking performance – mean Average Recall
  (mAR) and resource usage. With respect to a baseline with full
  Katch-Up usage and maximum channel usage, we still increase
  mAR by 4% while using 50% less of the channel and 30% power
  resources associated with Katch-Up. With respect to a fixed
  strategy using minimal resources, we increase mAR by 20% while
  using Katch-Up on 1/3 of the frames.},

  title={SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection},
  author={Callegaro, Davide and Restuccia, Francesco and Levorato, Marco},
  journal={arXiv preprint arXiv:2201.04235},
  html={https://arxiv.org/abs/2201.04235},
  year={2022}
}



@article{matsubara2022bottlefit,
  abbr={Preprint},
  abstract={Although mission-critical applications require the
  use of deep neural networks (DNNs), their continuous execution
  at mobile devices results in a significant increase in energy con-
  sumption. While edge offloading can decrease energy consump-
  tion, erratic patterns in channel quality, network and edge server
  load can lead to severe disruption of the system’s key operations.
  An alternative approach, called split computing, generates com-
  pressed representations within the model (called “bottlenecks”),
  to reduce bandwidth usage and energy consumption. Prior work
  has proposed approaches that introduce additional layers, to the
  detriment of energy consumption and latency. For this reason,
  we propose a new framework called BottleFit, which, in
  addition to targeted DNN architecture modifications, includes
  a novel training strategy to achieve high accuracy even with
  strong compression rates. We apply BottleFit on cutting-edge
  DNN models in image classification, and show that BottleFit
  achieves 77.1% data compression with up to 0.6% accuracy loss
  on ImageNet dataset, while state of the art such as SPINN loses
  up to 6% in accuracy. We experimentally measure the power
  consumption and latency of an image classification application
  running on an NVIDIA Jetson Nano board (GPU-based) and
  a Raspberry PI board (GPU-less). We show that BottleFit
  decreases power consumption and latency respectively by up to
  49% and 89% with respect to (w.r.t.) local computing and by 37%
  and 55% w.r.t. edge offloading. We also compare BottleFit
  with state-of-the-art autoencoders-based approaches, and show
  that (i) BottleFit reduces power consumption and execution
  time respectively by up to 54% and 44% on the Jetson and 40%
  and 62% on Raspberry PI; (ii) the size of the head model executed
  on the mobile device is 83 times smaller. The code repository will
  be published for full reproducibility of the results.},

  title={BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing},
  author={Matsubara, Yoshitomo and Callegaro, Davide and Singh, Sameer and Levorato, Marco and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.02693},
  html={https://arxiv.org/abs/2201.02693},
  year={2022}
}

@article{bahadori2022rewis,
  abbr={Preprint },
  abstract={Thanks to the ubiquitousness of Wi-Fi access points
  and devices, Wi-Fi sensing enables transformative applications
  in remote health care, home/office security, and surveillance, just
  to name a few. Existing work has explored the usage of machine
  learning (ML) on channel state information (CSI) computed from
  Wi-Fi packets to classify events of interest. However, most of
  these algorithms require a significant amount of data collection,
  as well as extensive computational power for additional CSI
  feature extraction. Moreover, the majority of these models suffer
  from poor accuracy when tested in a new/untrained environment.
  In this paper, we propose ReWiS, a novel framework for
  robust and environment-independent Wi-Fi sensing. The key
  innovation of ReWiS is to leverage few-shot learning (FSL) as
  the inference engine, which (i) reduces the need for extensive
  data collection and application-specific feature extraction; (ii)
  can rapidly generalize to new tasks by leveraging only a few
  new samples. Moreover, ReWiS leverages multi-antenna, multi-
  receiver diversity, as well as fine-grained frequency resolution,
  to improve the overall robustness of the algorithms. Finally, we
  propose a technique based on singular value decomposition (SVD)
  to make the FSL input constant irrespective of the number of
  receiver antennas. We prototype ReWiS using off-the-shelf Wi-
  Fi equipment and showcase its performance by considering a
  compelling use case of human activity recognition. Thus, we
  perform an extensive data collection campaign in three different
  propagation environments with two human subjects. We evaluate
  the impact of each diversity component on the performance
  and compare ReWiS with a traditional convolutional neural
  network (CNN) approach. Experimental results show that ReWiS
  improves the performance by about 40% with respect to exist-
  ing single-antenna low-resolution approaches. Moreover, when
  compared to a CNN-based approach, ReWiS shows 35% more
  accuracy and less than 10% drop in accuracy when tested in
  different environments, while the CNN drops by more than 45%.
  To allow reproducibility of our results and to address the current
  dearth of Wi-Fi sensing datasets, we pledge to release our 60 GB
  dataset and the entire code repository to the community.},
  title={ReWiS: Reliable Wi-Fi Sensing Through Few-Shot Multi-Antenna Multi-Receiver CSI Learning},
  author={Bahadori, Niloofar and Ashdown, Jonathan and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.00869},
  html={https://arxiv.org/abs/2201.00869},
  year={2022}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vegni2021series,
  abbr={Journal},
  abstract={In late 2019, a new virus was discovered, namely SARS-CoV-2. This strain causes severe acute respiratory syndrome coronavirus 2, defined as COVID-19. The disease soon spread all over the world, thus becoming a pandemic. It has been almost two years since worldwide restrictions on our lives started, and the traditional way people live and work has completely changed.},
  title={Series Editorial: Networking Technologies to Combat the COVID-19 Pandemic},
  author={Vegni, Anna Maria and Loscr{\`\i}, Valeria and Restuccia, Francesco and Yang, De-Nian},
  journal={IEEE Communications Magazine},
  volume={59},
  number={9},
  pages={14--15},
  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566516},
  year={2021},
  publisher={IEEE}
}


@article{restuccia2021deepfir,
  abbr={Journal},
  abstract={Deep learning can be used to classify waveform characteristics ( e.g. , modulation) with accuracy levels that are hardly attainable with traditional techniques. Recent research has demonstrated that one of the most crucial challenges in wireless deep learning is to counteract the channel action, which may significantly alter the waveform features. The problem is further exacerbated by the fact that deep learning algorithms are hardly re-trainable in real time due to their sheer size. This paper proposes DeepFIR , a framework to counteract the channel action in wireless deep learning algorithms without retraining the underlying deep learning model . The key intuition is that through the application of a carefully-optimized digital finite input response filter (FIR) at the transmitter’s side, we can apply tiny modifications to the waveform to strengthen its features according to the current channel conditions. We mathematically formulate the Waveform Optimization Problem (WOP) as the problem of finding the optimum FIR to be used on a waveform to improve the classifier’s accuracy. We also propose a data-driven methodology to train the FIRs directly with dataset inputs. We extensively evaluate DeepFIR on an experimental testbed of 20 software-defined radios, as well as on two datasets made up by 500 ADS-B devices and by 500 WiFi devices and a 24-class modulation dataset. Experimental results show that our approach (i) increases the accuracy of the radio fingerprinting models by about 35%, 50% and 58%; (ii) decreases an adversary’s accuracy by about 54% when trying to imitate other device’s fingerprints by using their filters; (iii) achieves 27% improvement over the state of the art on a 100-device dataset; (iv) increases by 2× the accuracy of the modulation dataset.},

  title={DeepFIR: Channel-Robust Physical-Layer Deep Learning Through Adaptive Waveform Filtering},
  author={Restuccia, Francesco and D’Oro, Salvatore and Al-Shawabka, Amani and Rendon, Bruno Costa and Ioannidis, Stratis and Melodia, Tommaso},
  journal={IEEE Transactions on Wireless Communications},
  volume={20},
  number={12},
  pages={8054--8066},
  html={https://ieeexplore.ieee.org/abstract/document/9470953},
  year={2021},
  publisher={IEEE}
}



@article{d2021coordinated,
  abbr={Journal},

  abstract={Radio access network (RAN) slicing is a virtualization technology that partitions radio resources into multiple autonomous virtual networks. Since RAN slicing can be tailored to provide diverse performance requirements, it will be pivotal to achieve the high-throughput and low-latency communications that next-generation (5G) systems have long yearned for. To this end, effective RAN slicing algorithms must (i) partition radio resources so as to leverage coordination among multiple base stations and thus boost network throughput; and (ii) reduce interference across different slices to guarantee slice isolation and avoid performance degradation. The ultimate goal of this paper is to design RAN slicing algorithms that address the above two requirements. First, we show that the RAN slicing problem can be formulated as a 0-1 Quadratic Programming problem, and we prove its NP-hardness. Second, we propose an optimal solution for small-scale 5G network deployments, and we present three approximation algorithms to make the optimization problem tractable when the network size increases. We first analyze the performance of our algorithms through simulations, and then demonstrate their performance through experiments on a standard-compliant LTE testbed with 2 base stations and 6 smartphones. Our results show that not only do our algorithms efficiently partition RAN resources, but also improve network throughput by 27% and increase by 2× the signal-to-interference-plus-noise ratio.},


  title={Coordinated 5G network slicing: How constructive interference can boost network throughput},
  author={D’Oro, Salvatore and Bonati, Leonardo and Restuccia, Francesco and Melodia, Tommaso},
  journal={IEEE/ACM Transactions on Networking},
  volume={29},
  number={4},
  pages={1881--1894},
  year={2021},

  html={https://ieeexplore.ieee.org/abstract/document/9411723},
  publisher={IEEE}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{tehrani2021federated,
  abbr={Conference},
  abstract={Next Generation (NextG) networks are expected to
  support demanding tactile internet applications such as aug-
  mented reality and connected autonomous vehicles. Whereas
  recent innovations bring the promise of larger link capacity,
  their sensitivity to the environment and erratic performance
  defy traditional model-based control rationales. Zero-touch data-
  driven approaches can improve the ability of the network
  to adapt to the current operating conditions. Tools such as
  reinforcement learning (RL) algorithms can build optimal control
  policy solely based on a history of observations. Specifically,
  deep RL (DRL), which uses a deep neural network (DNN)
  as a predictor, has been shown to achieve good performance
  even in complex environments and with high dimensional inputs.
  However, the training of DRL models require a large amount of
  data, which may limit its adaptability to ever-evolving statistics
  of the underlying environment. Moreover, wireless networks
  are inherently distributed systems, where centralized DRL ap-
  proaches would require excessive data exchange, while fully
  distributed approaches may result in slower convergence rates
  and performance degradation. In this paper, to address these
  challenges, we propose a federated learning (FL) approach to
  DRL, which we refer to federated DRL (F-DRL), where base
  stations (BS) collaboratively train the embedded DNN by only
  sharing models’ weights rather than training data. We evaluate
  two distinct versions of F-DRL, value and policy based, and show
  the superior performance they achieve compared to distributed
  and centralized DRL},

  title={Federated Deep Reinforcement Learning for the Distributed Control of NextG Wireless Networks},
  author={Tehrani, Peyman and Restuccia, Francesco and Levorato, Marco},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={248--253},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677132},
  organization={IEEE}
}

@inproceedings{bonati2021colosseum,
  abbr={Conference},
  abstract={Colosseum is an open-access and publicly-available
  large-scale wireless testbed for experimental research via virtu-
  alized and softwarized waveforms and protocol stacks on a fully
  programmable, “white-box” platform. Through 256 state-of-the-
  art software-defined radios and a massive channel emulator core,
  Colosseum can model virtually any scenario, enabling the de-
  sign, development and testing of solutions at scale in a variety
  of deployments and channel conditions. These Colosseum radio-
  frequency scenarios are reproduced through high-fidelity FPGA-
  based emulation with finite-impulse response filters. Filters model
  the taps of desired wireless channels and apply them to the signals
  generated by the radio nodes, faithfully mimicking the conditions
  of real-world wireless environments. In this paper, we introduce
  Colosseum as a testbed that is for the first time open to the research
  community. We describe the architecture of Colosseum and its
  experimentation and emulation capabilities. We then demonstrate
  the effectiveness of Colosseum for experimental research at scale
  through exemplary use cases including prevailing wireless tech-
  nologies (e.g., cellular and Wi-Fi) in spectrum sharing and un-
  manned aerial vehicle scenarios. A roadmap for Colosseum future
  updates concludes the paper.},

  title={Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation},
  author={Bonati, Leonardo and Johari, Pedram and Polese, Michele and D’Oro, Salvatore and Mohanti, Subhramoy and Tehrani-Moayyed, Miead and Villa, Davide and Shrivastava, Shweta and Tassie, Chinenye and Yoder, Kurt and others},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={105--113},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677430},
  organization={IEEE}
}


@inproceedings{al2021deeplora,
  abbr={Conference},
  abstract={

  The Long Range (LoRa) protocol for low-power wide-area networks (LPWANs) is a strong candidate to enable the massive roll-out of the Internet of Things (IoT) because of its low cost, impressive sensitivity (-137dBm), and massive scalability potential. As tens of thousands of tiny LoRa devices are deployed over large geographic areas, a key component to the success of LoRa will be the development of reliable and robust authentication mechanisms. To this end, Radio Frequency Fingerprinting (RFFP) through deep learning (DL) has been heralded as an effective zero-power supplement or alternative to energy-hungry cryptography. Existing work on LoRa RFFP has mostly focused on small-scale testbeds and low-dimensional learning techniques; however, many challenges remain. Key among them are authentication techniques robust to a wide variety of channel variations over time and supporting a vast population of devices.

  In this work, we advance the state of the art by presenting (i) the first massive experimental evaluation of DL RFFP and (ii) new data augmentation techniques for LoRa designed to counter the degradation introduced by the wireless channel. Specifically, we collected and publicly shared more than 1TB of waveform data from 100 bit-similar devices (with identical manufacturing processes) over different deployment scenarios (outdoor vs. indoor) and spanning several days. We train and test diverse DL models (convolutional and recurrent neural networks) using either preamble or payload data slices. We compare three different representations of the received signal: (i) IQ, (ii) amplitude-phase, and (iii) spectrogram. Finally, we propose a novel data augmentation technique called DeepLoRa to enhance the LoRa RFFP performance. Results show that (i) training the CNN models with IQ representation is not always the best combo in fingerprinting LoRa radios; training CNNs and RNN-LSTMs with amplitude-phase and spectrogram representations may increase the fingerprinting performance in small and medium-scale testbeds; (ii) using only payload data in the fingerprinting process outperforms preamble only data, and (iii) DeepLoRa data augmentation technique improves the classification accuracy from 19% to 36% in the RFFP challenging case of training on data collected on a different day than the testing data. Moreover, DeepLoRa raises the accuracy from 82% to 91% when training and testing 100 devices with data collected on the same day.},
  title={DeepLoRa: Fingerprinting LoRa Devices at Scale Through Deep Learning and Data Augmentation},

  author={Al-Shawabka, Amani and Pietraski, Philip and Pattar, Sudhir B and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={251--260},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467054},
  year={2021}
}



@inproceedings{piva2021tags,
  abbr={Conference},
  abstract={Millions of RFID tags are pervasively used all around the globe to inexpensively identify a wide variety of everyday-use objects. One of the key issues of RFID is that tags cannot use energy-hungry cryptography, and thus can be easily cloned. For this reason, radio fingerprinting (RFP) is a compelling approach that leverages the unique imperfections in the tag's wireless circuitry to achieve large-scale RFID clone detection. Recent work, however, has unveiled that time-varying channel conditions can significantly decrease the accuracy of the RFP process. Prior art in RFID identification does not consider this critical aspect, and instead focuses on custom-tailored feature extraction techniques and data collection with static channel conditions. For this reason, we propose the first large-scale investigation into RFP of RFID tags with dynamic channel conditions. Specifically, we perform a massive data collection campaign on a testbed composed by 200 off-the-shelf identical RFID tags and a software-defined radio (SDR) tag reader. We collect data with different tag-reader distances in an over-the-air configuration. To emulate implanted RFID tags, we also collect data with two different kinds of porcine meat inserted between the tag and the reader. We use this rich dataset to train and test several convolutional neural network (CNN)-based classifiers in a variety of channel conditions. Our investigation reveals that training and testing on different channel conditions drastically degrades the classifier's accuracy. For this reason, we propose a novel training framework based on federated machine learning (FML) and data augmentation (DAG) to boost the accuracy. Extensive experimental results indicate that (i) our FML approach improves accuracy by up to 48%; (ii) our DAG approach improves the FML performance by up to 19% and the single-dataset performance by 31%. To the best of our knowledge, this is the first paper experimentally demonstrating the efficacy of FML and DAG on a large device population. To allow full replicability, we are sharing with the research community our fully-labeled 200-GB RFID waveform dataset, as well as the entirety of our code and trained models, concurrently with our submission.},

  title={The Tags Are Alright: Robust Large-Scale RFID Clone Detection Through Federated Data-Augmented Radio Fingerprinting},
  author={Piva, Mauro and Maselli, Gaia and Restuccia, Francesco},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467033},
  pages={41--50},
  year={2021}
}


@inproceedings{polese2021deepbeam,
  abbr={Conference},
  abstract={Highly directional millimeter wave (mmWave) radios need to perform beam management to establish and maintain reliable links. To achieve this objective, existing solutions mostly rely on explicit coordination between the transmitter (TX) and the receiver (RX), which significantly reduces the airtime available for communication and further complicates the network protocol design. This paper advances the state of the art by presenting DeepBeam, a framework for beam management that does not require pilot sequences from the TX, nor any beam sweeping or synchronization from the RX. This is achieved by inferring (i) the Angle of Arrival (AoA) of the beam and (ii) the actual beam being used by the transmitter through waveform-level deep learning on ongoing transmissions between the TX to other receivers. In this way, the RX can associate Signal-to-Noise-Ratio (SNR) levels to beams without explicit coordination with the TX. This is possible because different beam patterns introduce different "impairments" to the waveform, which can be subsequently learned by a convolutional neural network (CNN). To demonstrate the generality of DeepBeam, we conduct an extensive experimental data collection campaign where we collect more than 4 TB of mmWave waveforms with (i) 4 phased array antennas at 60.48 GHz, (ii) 2 codebooks containing 24 one-dimensional beams and 12 two-dimensional beams; (iii) 3 receiver gains; (iv) 3 different AoAs; (v) multiple TX and RX locations. Moreover, we collect waveform data with two custom-designed mmWave software-defined radios with fully-digital beamforming architectures at 58 GHz. We also implement our learning models in FPGA to evaluate latency performance. Results show that DeepBeam (i) achieves accuracy of up to 96%, 84% and 77% with a 5-beam, 12-beam and 24-beam codebook, respectively; (ii) reduces latency by up to 7x with respect to the 5G NR initial beam sweep in a default configuration and with a 12-beam codebook. The waveform dataset and the full DeepBeam code repository are publicly available.},

  title={DeepBeam: Deep Waveform Learning for Coordination-Free Beam Management in mmWave Networks},
  author={Polese, Michele and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467035},
  pages={61--70},
  year={2021}
}


@inproceedings{ghiro2021blockchain,
  abbr={Conference},
  abstract={The term blockchain is used for disparate projects, ranging from cryptocurrencies to applications for the Internet of Things (IoT). The concept of blockchain appears therefore blurred, as the same technology cannot empower applications with extremely different requirements, levels of security and performance. This position paper elaborates on the theory of distributed systems to advance a clear definition of blockchain allowing us to clarify its possible role in the IoT. The definition binds together three elements that, as a whole, delineate those unique features that distinguish the blockchain from other distributed ledger technologies: immutability, transparency and anonymity. We note that immutability-which is imperative for securing blockchains-imposes remarkable resource consumption. Moreover, while transparency demands no confidentiality, anonymity enhances privacy but prevents user identification. As such, we raise the concern that these blockchain features clash with the requirements of most IoT applications where devices are power-constrained, data needs to be kept confidential, and users to be clearly identifiable. We consequently downplay the role of the blockchain for the IoT: this can act as a ledger external to the IoT architecture, invoked as seldom as possible and only to record the aggregate results of myriads of local (IoT) transactions that are most of the time performed off-chain to meet performance and scalability requirements.},
  
  title={A Blockchain Definition to Clarify its Role for the Internet of Things},

  author={Ghiro, Lorenzo and Restuccia, Francesco and D'Oro, Salvatore and Basagni, Stefano and Melodia, Tommaso and Maccari, Leonardo and Cigno, Renato Lo},
  booktitle={2021 19th Mediterranean Communication and Computer Networking Conference (MedComNet)},
  pages={1--8},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9501280},
  organization={IEEE}
}



@inproceedings{uvaydov2021deepsense,
  abbr={Conference},

  abstract={Spectrum sharing will be a key technology to tackle
  spectrum scarcity in the sub-6 GHz bands. To fairly access the
  shared bandwidth, wireless users will necessarily need to quickly
  sense large portions of spectrum and opportunistically access
  unutilized bands. The key unaddressed challenges of spectrum
  sensing are that (i) it has to be performed with extremely low
  latency over large bandwidths to detect tiny spectrum holes and
  to guarantee strict real-time digital signal processing (DSP) con-
  straints; (ii) its underlying algorithms need to be extremely accu-
  rate, and flexible enough to work with different wireless bands
  and protocols to find application in real-world settings. To the
  best of our knowledge, the literature lacks spectrum sensing tech-
  niques able to accomplish both requirements. In this paper, we
  propose DeepSense, a software/hardware framework for real-time
  wideband spectrum sensing that relies on real-time deep learn-
  ing tightly integrated into the transceiver’s baseband processing
  logic to detect and exploit unutilized spectrum bands. DeepSense
  uses a convolutional neural network (CNN) implemented in the
  wireless platform’s hardware fabric to analyze a small portion of
  the unprocessed baseband waveform to automatically extract the
  maximum amount of information with the least amount of I/Q
  samples. We extensively validate the accuracy, latency and gen-
  erality performance of DeepSense with (i) a 400 GB dataset con-
  taining hundreds of thousands of WiFi transmissions collected “in
  the wild” with different Signal-to-Noise-Ratio (SNR) conditions
  and over different days; (ii) a dataset of transmissions collected
  using our own software-defined radio testbed; and (iii) a synthetic
  dataset of LTE transmissions under controlled SNR conditions.
  We also measure the real-time latency of the CNNs trained on
  the three datasets with an FPGA implementation, and compare
  our approach with a fixed energy threshold mechanism. Results
  show that our learning-based approach can deliver a precision
  and recall of 98% and 97% respectively and a latency as low
  as 0.61ms. For reproducibility and benchmarking purposes, we
  pledge to share the code and the datasets used in this paper to
  the community.},

  title={Deepsense: Fast wideband spectrum sensing through real-time in-the-loop deep learning},
  author={Uvaydov, Daniel and D’Oro, Salvatore and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},

  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9488764},
  year={2021},
  organization={IEEE}
}

@inproceedings{d2021can,
  abbr={Conference},
  abstract= {Due to the sheer scale of the Internet of Things (IoT)
  and 5G, the wireless spectrum is becoming severely congested.
  For this reason, wireless devices will need to continuously adapt
  to current spectrum conditions by changing their communication
  parameters in real-time. Therefore, wireless signal classification
  (WSC) will become a compelling necessity to decode fast-changing
  signals from dynamic transmitters. Thanks to its capability of
  classifying complex phenomena without explicit mathematical
  modeling, deep learning (DL) has been demonstrated to be a key
  enabler of WSC. Although DL can achieve a very high accuracy
  under certain conditions, recent research has unveiled that the
  wireless channel can disrupt the features learned by the DL model
  during training, thus drastically reducing the classification per-
  formance in real-world live settings. Since retraining classifiers
  is cumbersome after deployment, existing work has leveraged
  the usage of carefully-tailored Finite Impulse Response (FIR)
  filters that, when applied at the transmitter’s side, can restore
  the features that are lost because of the the channel actions,
  i.e., waveform synthesis. However, these approaches compute FIRs
  using offline optimization strategies, which limits their efficacy
  in highly-dynamic channel settings. In this paper, we improve
  the state of the art by proposing Chares, a Deep Reinforcement
  Learning (DRL)-based framework for channel-resilient adaptive
  waveform synthesis. Chares adapts to new and unseen channel
  conditions by optimally computing through DRL the FIRs in real
  time. Chares is a DRL agent whose architecture is based upon
  the Twin Delayed Deep Deterministic Policy Gradients (TD3),
  which requires minimal feedback from the receiver and explores a
  continuous action space for best performance. Chares has been ex-
  tensively evaluated on two well-known datasets with an extensive
  number of channels. We have also evaluated the real-time latency
  of Chares with an implementation on field-programmable gate
  array (FPGA). Results show that Chares increases the accuracy
  up to 4.1x when no waveform synthesis is performed, by 1.9x with
  respect to existing work, and can compute new actions within 41μs.},

  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9488865},
  title={Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for Resilient Wireless Signal Classification},
  author={D’Oro, Salvatore and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2021},
  organization={IEEE}
}


@inproceedings{bonati2021stealte,
  abbr={Conference},
  abstract={Fifth-generation (5G) systems will extensively employ radio access network (RAN) softwarization. This key innovation enables the instantiation of "virtual cellular networks" running on different slices of the shared physical infrastructure. In this paper, we propose the concept of Private Cellular Connectivity as a Service (PCCaaS), where infrastructure providers deploy covert network slices known only to a subset of users. We then present SteaLTE as the first realization of a PCCaaS-enabling system for cellular networks. At its core, SteaLTE utilizes wireless steganography to disguise data as noise to adversarial receivers. Differently from previous work, however, it takes a full-stack approach to steganography, contributing an LTE-compliant stegano-graphic protocol stack for PCCaaS-based communications, and packet schedulers and operations to embed covert data streams on top of traditional cellular traffic (primary traffic). SteaLTE balances undetectability and performance by mimicking channel impairments so that covert data waveforms are almost indistinguishable from noise. We evaluate the performance of SteaLTE on an indoor LTE-compliant testbed under different traffic profiles, distance and mobility patterns. We further test it on the outdoor PAWR POWDER platform over long-range cellular links. Results show that in most experiments SteaLTE imposes little loss of primary traffic throughput in presence of covert data transmissions (<; 6%), making it suitable for undetectable PCCaaS networking.},

  title={SteaLTE: Private 5G Cellular Connectivity as a Service with Full-stack Wireless Steganography},
  author={Bonati, Leonardo and D’Oro, Salvatore and Restuccia, Francesco and Basagni, Stefano and Melodia, Tommaso},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9488889},
  organization={IEEE}
}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

















