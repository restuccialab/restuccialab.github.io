---
---


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{restuccia2022toward,
  abbr={Journal},
  abstract={Wireless systems such as the Internet of Things (IoT) are changing the way we interact with the cyber and the physical world. As IoT systems become more and more pervasive, it is imperative to design wireless protocols that can effectively and efficiently support IoT devices and operations. On the other hand, today's IoT wireless systems are based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. In this paper, we introduce the new notion of a deep learning-based polymorphic IoT receiver, able to reconfigure its waveform demodulation strategy itself in real time, based on the inferred waveform parameters. Our key innovation is the introduction of a novel embedded deep learning architecture that enables the solution of waveform inference problems, which is then integrated into a generalized hardware/software architecture with radio components and signal processing. Our polymorphic wireless receiver is prototyped on a custom-made software-defined radio platform. We show through extensive over-the-air experiments that the system achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible.},


  title={Toward Polymorphic Internet of Things Receivers Through Real-Time Waveform-Level Deep Learning},
  author={Restuccia, Francesco and Melodia, Tommaso},
  journal={GetMobile: Mobile Computing and Communications},
  volume={25},
  html={https://dl.acm.org/doi/abs/10.1145/3511285.3511294?casa_token=ziKh5Pvf8NIAAAAA:OXKzC4Ck_4f47V0oKB1lBQxdXshK7uISr859oeAI_u3BytSo5ZAeeirEoQjXMBmCkaUKS4GkMNQHEQ},
  number={3},
  pages={28--33},
  year={2022},
  publisher={ACM New York, NY, USA}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@article{baldesi2022charm,
  abbr={Preprint},
  abstract={oday’s radio access networks (RANs) are monolithic
  entities which often operate statically on a given set of parameters
  for the entirety of their operations. To implement realistic and
  effective spectrum sharing policies, RANs will need to seamlessly
  and intelligently change their operational parameters. In stark
  contrast with existing paradigms, the new O-RAN architectures
  for 5G-and-beyond networks (NextG) separate the logic that con-
  trols the RAN from its hardware substrate, allowing unprece-
  dented real-time fine-grained control of RAN components. In
  this context, we propose the Channel-Aware Reactive Mechanism
  (ChARM), a data-driven O-RAN-compliant framework that allows
  (i) sensing the spectrum to infer the presence of interference
  and (ii) reacting in real time by switching the distributed unit
  (DU) and radio unit (RU) operational parameters according to
  a specified spectrum access policy. ChARM is based on neural
  networks operating directly on unprocessed I/Q waveforms to
  determine the current spectrum context. ChARM does not require
  any modification to the existing 3GPP standards. It is designed
  to operate within the O-RAN specifications, and can be used in
  conjunction with other spectrum sharing mechanisms (e.g., LTE-
  U, LTE-LAA or MulteFire). We demonstrate the performance of
  ChARM in the context of spectrum sharing among LTE and Wi-
  Fi in unlicensed bands, where a controller operating over a RAN
  Intelligent Controller (RIC) senses the spectrum and switches
  cell frequency to avoid Wi-Fi. We develop a prototype of ChARM
  using srsRAN, and leverage the Colosseum channel emulator to
  collect a large-scale waveform dataset to train our neural networks
  with. To collect standard-compliant Wi-Fi data, we extended the
  Colosseum testbed using system-on-chip (SoC) boards running
  a modified version of the OpenWiFi architecture. Experimental
  results show that ChARM achieves accuracy of up to 96% on
  Colosseum and 85% on an over-the-air testbed, demonstrating the
  capacity of ChARM to exploit the considered spectrum channels},

  title={ChARM: NextG Spectrum Sharing Through Data-Driven Real-Time O-RAN Dynamic Control},
  author={Baldesi, Luca and Restuccia, Francesco and Melodia, Tommaso},
  journal={arXiv preprint arXiv:2201.06326},
  html={https://arxiv.org/abs/2201.06326},
  year={2022}
}

@article{callegaro2022smartdet,
  abbr={Preprint},
  abstract={Mobile devices increasingly rely on object detec-
  tion (OD) through deep neural networks (DNNs) to perform
  critical tasks. Due to their high complexity, the execution of
  these DNNs requires excessive time and energy. Low-complexity
  object tracking (OT) can be used with OD, where the latter is
  periodically applied to generate “fresh” references for tracking.
  However, the frames processed with OD incur large delays,
  which may make the reference outdated and degrade tracking
  quality. Herein, we propose to use edge computing in this
  context, and establish parallel OT (at the mobile device) and
  OD (at the edge server) processes that are resilient to large OD
  latency. We propose Katch-Up, a novel tracking mechanism that
  improves the system resilience to excessive OD delay. However,
  while Katch-Up significantly improves performance, it also
  increases the computing load of the mobile device. Hence, we
  design SmartDet, a low-complexity controller based on deep
  reinforcement learning (DRL) that learns controlling the trade-
  off between resource utilization and OD performance. SmartDet
  takes as input context-related information related to the current
  video content and the current network conditions to optimize
  frequency and type of OD offloading, as well as Katch-Up
  utilization. We extensively evaluate SmartDet on a real-world
  testbed composed of a JetSon Nano as mobile device and a
  GTX 980 Ti as edge server, connected through a Wi-Fi link.
  Experimental results show that SmartDet achieves an optimal
  balance between tracking performance – mean Average Recall
  (mAR) and resource usage. With respect to a baseline with full
  Katch-Up usage and maximum channel usage, we still increase
  mAR by 4% while using 50% less of the channel and 30% power
  resources associated with Katch-Up. With respect to a fixed
  strategy using minimal resources, we increase mAR by 20% while
  using Katch-Up on 1/3 of the frames.},

  title={SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection},
  author={Callegaro, Davide and Restuccia, Francesco and Levorato, Marco},
  journal={arXiv preprint arXiv:2201.04235},
  html={https://arxiv.org/abs/2201.04235},
  year={2022}
}



@article{matsubara2022bottlefit,
  abbr={Preprint},
  abstract={Although mission-critical applications require the
  use of deep neural networks (DNNs), their continuous execution
  at mobile devices results in a significant increase in energy con-
  sumption. While edge offloading can decrease energy consump-
  tion, erratic patterns in channel quality, network and edge server
  load can lead to severe disruption of the system’s key operations.
  An alternative approach, called split computing, generates com-
  pressed representations within the model (called “bottlenecks”),
  to reduce bandwidth usage and energy consumption. Prior work
  has proposed approaches that introduce additional layers, to the
  detriment of energy consumption and latency. For this reason,
  we propose a new framework called BottleFit, which, in
  addition to targeted DNN architecture modifications, includes
  a novel training strategy to achieve high accuracy even with
  strong compression rates. We apply BottleFit on cutting-edge
  DNN models in image classification, and show that BottleFit
  achieves 77.1% data compression with up to 0.6% accuracy loss
  on ImageNet dataset, while state of the art such as SPINN loses
  up to 6% in accuracy. We experimentally measure the power
  consumption and latency of an image classification application
  running on an NVIDIA Jetson Nano board (GPU-based) and
  a Raspberry PI board (GPU-less). We show that BottleFit
  decreases power consumption and latency respectively by up to
  49% and 89% with respect to (w.r.t.) local computing and by 37%
  and 55% w.r.t. edge offloading. We also compare BottleFit
  with state-of-the-art autoencoders-based approaches, and show
  that (i) BottleFit reduces power consumption and execution
  time respectively by up to 54% and 44% on the Jetson and 40%
  and 62% on Raspberry PI; (ii) the size of the head model executed
  on the mobile device is 83 times smaller. The code repository will
  be published for full reproducibility of the results.},

  title={BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing},
  author={Matsubara, Yoshitomo and Callegaro, Davide and Singh, Sameer and Levorato, Marco and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.02693},
  html={https://arxiv.org/abs/2201.02693},
  year={2022}
}

@article{bahadori2022rewis,
  abbr={Preprint },
  abstract={Thanks to the ubiquitousness of Wi-Fi access points
  and devices, Wi-Fi sensing enables transformative applications
  in remote health care, home/office security, and surveillance, just
  to name a few. Existing work has explored the usage of machine
  learning (ML) on channel state information (CSI) computed from
  Wi-Fi packets to classify events of interest. However, most of
  these algorithms require a significant amount of data collection,
  as well as extensive computational power for additional CSI
  feature extraction. Moreover, the majority of these models suffer
  from poor accuracy when tested in a new/untrained environment.
  In this paper, we propose ReWiS, a novel framework for
  robust and environment-independent Wi-Fi sensing. The key
  innovation of ReWiS is to leverage few-shot learning (FSL) as
  the inference engine, which (i) reduces the need for extensive
  data collection and application-specific feature extraction; (ii)
  can rapidly generalize to new tasks by leveraging only a few
  new samples. Moreover, ReWiS leverages multi-antenna, multi-
  receiver diversity, as well as fine-grained frequency resolution,
  to improve the overall robustness of the algorithms. Finally, we
  propose a technique based on singular value decomposition (SVD)
  to make the FSL input constant irrespective of the number of
  receiver antennas. We prototype ReWiS using off-the-shelf Wi-
  Fi equipment and showcase its performance by considering a
  compelling use case of human activity recognition. Thus, we
  perform an extensive data collection campaign in three different
  propagation environments with two human subjects. We evaluate
  the impact of each diversity component on the performance
  and compare ReWiS with a traditional convolutional neural
  network (CNN) approach. Experimental results show that ReWiS
  improves the performance by about 40% with respect to exist-
  ing single-antenna low-resolution approaches. Moreover, when
  compared to a CNN-based approach, ReWiS shows 35% more
  accuracy and less than 10% drop in accuracy when tested in
  different environments, while the CNN drops by more than 45%.
  To allow reproducibility of our results and to address the current
  dearth of Wi-Fi sensing datasets, we pledge to release our 60 GB
  dataset and the entire code repository to the community.},
  title={ReWiS: Reliable Wi-Fi Sensing Through Few-Shot Multi-Antenna Multi-Receiver CSI Learning},
  author={Bahadori, Niloofar and Ashdown, Jonathan and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2201.00869},
  html={https://arxiv.org/abs/2201.00869},
  year={2022}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2021   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Journals %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vegni2021series,
  abbr={Journal},
  abstract={In late 2019, a new virus was discovered, namely SARS-CoV-2. This strain causes severe acute respiratory syndrome coronavirus 2, defined as COVID-19. The disease soon spread all over the world, thus becoming a pandemic. It has been almost two years since worldwide restrictions on our lives started, and the traditional way people live and work has completely changed.},
  title={Series Editorial: Networking Technologies to Combat the COVID-19 Pandemic},
  author={Vegni, Anna Maria and Loscr{\`\i}, Valeria and Restuccia, Francesco and Yang, De-Nian},
  journal={IEEE Communications Magazine},
  volume={59},
  number={9},
  pages={14--15},
  html={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566516},
  year={2021},
  publisher={IEEE}
}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conferences %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{tehrani2021federated,
  abbr={Conference},
  abstract={Next Generation (NextG) networks are expected to
  support demanding tactile internet applications such as aug-
  mented reality and connected autonomous vehicles. Whereas
  recent innovations bring the promise of larger link capacity,
  their sensitivity to the environment and erratic performance
  defy traditional model-based control rationales. Zero-touch data-
  driven approaches can improve the ability of the network
  to adapt to the current operating conditions. Tools such as
  reinforcement learning (RL) algorithms can build optimal control
  policy solely based on a history of observations. Specifically,
  deep RL (DRL), which uses a deep neural network (DNN)
  as a predictor, has been shown to achieve good performance
  even in complex environments and with high dimensional inputs.
  However, the training of DRL models require a large amount of
  data, which may limit its adaptability to ever-evolving statistics
  of the underlying environment. Moreover, wireless networks
  are inherently distributed systems, where centralized DRL ap-
  proaches would require excessive data exchange, while fully
  distributed approaches may result in slower convergence rates
  and performance degradation. In this paper, to address these
  challenges, we propose a federated learning (FL) approach to
  DRL, which we refer to federated DRL (F-DRL), where base
  stations (BS) collaboratively train the embedded DNN by only
  sharing models’ weights rather than training data. We evaluate
  two distinct versions of F-DRL, value and policy based, and show
  the superior performance they achieve compared to distributed
  and centralized DRL},

  title={Federated Deep Reinforcement Learning for the Distributed Control of NextG Wireless Networks},
  author={Tehrani, Peyman and Restuccia, Francesco and Levorato, Marco},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={248--253},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677132},
  organization={IEEE}
}

@inproceedings{bonati2021colosseum,
  abbr={Conference},
  abstract={Colosseum is an open-access and publicly-available
  large-scale wireless testbed for experimental research via virtu-
  alized and softwarized waveforms and protocol stacks on a fully
  programmable, “white-box” platform. Through 256 state-of-the-
  art software-defined radios and a massive channel emulator core,
  Colosseum can model virtually any scenario, enabling the de-
  sign, development and testing of solutions at scale in a variety
  of deployments and channel conditions. These Colosseum radio-
  frequency scenarios are reproduced through high-fidelity FPGA-
  based emulation with finite-impulse response filters. Filters model
  the taps of desired wireless channels and apply them to the signals
  generated by the radio nodes, faithfully mimicking the conditions
  of real-world wireless environments. In this paper, we introduce
  Colosseum as a testbed that is for the first time open to the research
  community. We describe the architecture of Colosseum and its
  experimentation and emulation capabilities. We then demonstrate
  the effectiveness of Colosseum for experimental research at scale
  through exemplary use cases including prevailing wireless tech-
  nologies (e.g., cellular and Wi-Fi) in spectrum sharing and un-
  manned aerial vehicle scenarios. A roadmap for Colosseum future
  updates concludes the paper.},

  title={Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation},
  author={Bonati, Leonardo and Johari, Pedram and Polese, Michele and D’Oro, Salvatore and Mohanti, Subhramoy and Tehrani-Moayyed, Miead and Villa, Davide and Shrivastava, Shweta and Tassie, Chinenye and Yoder, Kurt and others},
  booktitle={2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
  pages={105--113},
  year={2021},
  html={https://ieeexplore.ieee.org/abstract/document/9677430},
  organization={IEEE}
}


@inproceedings{al2021deeplora,
  abbr={Conference},
  abstract={

  The Long Range (LoRa) protocol for low-power wide-area networks (LPWANs) is a strong candidate to enable the massive roll-out of the Internet of Things (IoT) because of its low cost, impressive sensitivity (-137dBm), and massive scalability potential. As tens of thousands of tiny LoRa devices are deployed over large geographic areas, a key component to the success of LoRa will be the development of reliable and robust authentication mechanisms. To this end, Radio Frequency Fingerprinting (RFFP) through deep learning (DL) has been heralded as an effective zero-power supplement or alternative to energy-hungry cryptography. Existing work on LoRa RFFP has mostly focused on small-scale testbeds and low-dimensional learning techniques; however, many challenges remain. Key among them are authentication techniques robust to a wide variety of channel variations over time and supporting a vast population of devices.

  In this work, we advance the state of the art by presenting (i) the first massive experimental evaluation of DL RFFP and (ii) new data augmentation techniques for LoRa designed to counter the degradation introduced by the wireless channel. Specifically, we collected and publicly shared more than 1TB of waveform data from 100 bit-similar devices (with identical manufacturing processes) over different deployment scenarios (outdoor vs. indoor) and spanning several days. We train and test diverse DL models (convolutional and recurrent neural networks) using either preamble or payload data slices. We compare three different representations of the received signal: (i) IQ, (ii) amplitude-phase, and (iii) spectrogram. Finally, we propose a novel data augmentation technique called DeepLoRa to enhance the LoRa RFFP performance. Results show that (i) training the CNN models with IQ representation is not always the best combo in fingerprinting LoRa radios; training CNNs and RNN-LSTMs with amplitude-phase and spectrogram representations may increase the fingerprinting performance in small and medium-scale testbeds; (ii) using only payload data in the fingerprinting process outperforms preamble only data, and (iii) DeepLoRa data augmentation technique improves the classification accuracy from 19% to 36% in the RFFP challenging case of training on data collected on a different day than the testing data. Moreover, DeepLoRa raises the accuracy from 82% to 91% when training and testing 100 devices with data collected on the same day.},
  title={DeepLoRa: Fingerprinting LoRa Devices at Scale Through Deep Learning and Data Augmentation},

  author={Al-Shawabka, Amani and Pietraski, Philip and Pattar, Sudhir B and Restuccia, Francesco and Melodia, Tommaso},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={251--260},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467054},
  year={2021}
}



@inproceedings{piva2021tags,
  abbr={Conference},
  abstract={Millions of RFID tags are pervasively used all around the globe to inexpensively identify a wide variety of everyday-use objects. One of the key issues of RFID is that tags cannot use energy-hungry cryptography, and thus can be easily cloned. For this reason, radio fingerprinting (RFP) is a compelling approach that leverages the unique imperfections in the tag's wireless circuitry to achieve large-scale RFID clone detection. Recent work, however, has unveiled that time-varying channel conditions can significantly decrease the accuracy of the RFP process. Prior art in RFID identification does not consider this critical aspect, and instead focuses on custom-tailored feature extraction techniques and data collection with static channel conditions. For this reason, we propose the first large-scale investigation into RFP of RFID tags with dynamic channel conditions. Specifically, we perform a massive data collection campaign on a testbed composed by 200 off-the-shelf identical RFID tags and a software-defined radio (SDR) tag reader. We collect data with different tag-reader distances in an over-the-air configuration. To emulate implanted RFID tags, we also collect data with two different kinds of porcine meat inserted between the tag and the reader. We use this rich dataset to train and test several convolutional neural network (CNN)-based classifiers in a variety of channel conditions. Our investigation reveals that training and testing on different channel conditions drastically degrades the classifier's accuracy. For this reason, we propose a novel training framework based on federated machine learning (FML) and data augmentation (DAG) to boost the accuracy. Extensive experimental results indicate that (i) our FML approach improves accuracy by up to 48%; (ii) our DAG approach improves the FML performance by up to 19% and the single-dataset performance by 31%. To the best of our knowledge, this is the first paper experimentally demonstrating the efficacy of FML and DAG on a large device population. To allow full replicability, we are sharing with the research community our fully-labeled 200-GB RFID waveform dataset, as well as the entirety of our code and trained models, concurrently with our submission.},
  title={The Tags Are Alright: Robust Large-Scale RFID Clone Detection Through Federated Data-Augmented Radio Fingerprinting},
  author={Piva, Mauro and Maselli, Gaia and Restuccia, Francesco},
  booktitle={Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  html={https://dl.acm.org/doi/abs/10.1145/3466772.3467033},
  pages={41--50},
  year={2021}
}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preprints %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

















